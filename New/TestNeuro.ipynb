{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a1a9ac-9054-4e31-9556-33b84f2761b6",
   "metadata": {},
   "source": [
    "### Импорт модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063be81d-bb64-42d2-9f5f-b68705da5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57424b-40cc-4df1-9058-6dcf86a41f02",
   "metadata": {},
   "source": [
    "### Попытка завести на GTX 1050. Но CUDA 11.6 а надо 11.8 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7114ea9f-2c4a-4e7a-8201-220b667aeb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc493d9-46a0-48fd-9e5b-b24bd959a649",
   "metadata": {},
   "source": [
    "### Задания констант для нейросети, обучения и протоколирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050b5041-3411-4667-9c63-6345b2f458bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYER1 = 128\n",
    "HIDDEN_LAYER2 = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "Episode = namedtuple('Episode', ['reward', 'reward_with_discount', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', ['state', 'action'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf605f89-4e96-4f38-97aa-e448ae622ed8",
   "metadata": {},
   "source": [
    "### Классы энвайромента и нейросети, а также функции создания батчей и их фильтрации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20846ae-a2dd-4000-860c-b37215294b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    # def __init__(self, obs_size, hidden_layer1, n_actions):\n",
    "    def __init__(self, obs_size, hidden_layer1, hidden_layer2, n_actions):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.Linear(obs_size, hidden_layer1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(hidden_layer1, n_actions),\n",
    "        # )\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_layer1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer1, hidden_layer2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer2, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def generate_batches_of_episodes(env, net, batch_size, actions_n):\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    batch = []\n",
    "\n",
    "    sm = nn.Softmax(dim=1)\n",
    "\n",
    "    # Reset the environment and capture the current state\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Use the neural network with random.choice to choose an action\n",
    "        state_t = torch.FloatTensor([state])\n",
    "        action_probs_t = sm(net(state_t))\n",
    "        action_probs = action_probs_t.data.numpy()[0]\n",
    "        action = np.random.choice(actions_n, p=action_probs)\n",
    "\n",
    "        # Apply a step using the chosen action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Add the reward to the total reward for this episode\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Record the state before the action was taken and the action itself\n",
    "        episode_steps.append(EpisodeStep(state=state, action=action))\n",
    "\n",
    "        # Check if the episode has ended\n",
    "        if terminated or truncated:\n",
    "\n",
    "            # Discount the total episode reward to create variability between episodes\n",
    "            episode_reward_with_discount = episode_reward * (DISCOUNT ** len(episode_steps))\n",
    "\n",
    "            # Record the episode\n",
    "            batch.append(Episode(reward=episode_reward, reward_with_discount=episode_reward_with_discount, steps=episode_steps))\n",
    "\n",
    "            # Reset vars\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_state, _ = env.reset()\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "\n",
    "                # Return the batch to the training loop\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "\n",
    "    # Set a threshold based on the n-th percentile of discounted episode rewards within the batch\n",
    "    episode_reward_threshold = np.percentile(list(map(lambda s: s.reward_with_discount, batch)), percentile)\n",
    "\n",
    "    best_episodes = []\n",
    "    batch_states = []\n",
    "    batch_actions = []\n",
    "\n",
    "    for episode in batch:\n",
    "        if episode.reward_with_discount > episode_reward_threshold:\n",
    "\n",
    "            # Add the states and actions from a high performing episode\n",
    "            batch_states.extend(map(lambda step: step.state, episode.steps))\n",
    "            batch_actions.extend(map(lambda step: step.action, episode.steps))\n",
    "\n",
    "            best_episodes.append(episode)\n",
    "\n",
    "    return best_episodes[-500:], torch.FloatTensor(batch_states), torch.LongTensor(batch_actions), episode_reward_threshold\n",
    "\n",
    "def visualize(env):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "def render_n_steps(env, net, steps_n):\n",
    "\n",
    "    sm = nn.Softmax(dim=1) \n",
    "    state, _ = env.reset()\n",
    "\n",
    "    for i in range(steps_n):\n",
    "\n",
    "        state_t = torch.FloatTensor([state])\n",
    "\n",
    "        if net is None:\n",
    "            # Choose a random step\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Choose a step using the (trained) neural network\n",
    "            action_probs_t = sm(net(state_t))\n",
    "            action = np.argmax(action_probs_t.data.numpy()[0])\n",
    "\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Render the step on the display\n",
    "        env.render()\n",
    "\n",
    "        if terminated or truncated: state, _ = env.reset()            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d80a17-7f73-43c2-9fd3-1b0af746b752",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9083c6f-be8e-43ac-ae7e-789264b007c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_16764\\3707084875.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  state_t = torch.FloatTensor([state])\n",
      "C:\\Users\\grego\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:\tLoss: 1.3719\tMean ep reward: 0.01\tMean ep reward with disc: 0.0019\n",
      "3:\tLoss: 1.3729\tMean ep reward: 0.01\tMean ep reward with disc: 0.0017\n",
      "4:\tLoss: 1.3617\tMean ep reward: 0.02\tMean ep reward with disc: 0.0079\n",
      "5:\tLoss: 1.3494\tMean ep reward: 0.02\tMean ep reward with disc: 0.0031\n",
      "6:\tLoss: 1.3469\tMean ep reward: 0.02\tMean ep reward with disc: 0.0027\n",
      "7:\tLoss: 1.3445\tMean ep reward: 0.02\tMean ep reward with disc: 0.0084\n",
      "8:\tLoss: 1.3538\tMean ep reward: 0.02\tMean ep reward with disc: 0.0035\n",
      "9:\tLoss: 1.3501\tMean ep reward: 0.02\tMean ep reward with disc: 0.0054\n",
      "10:\tLoss: 1.3531\tMean ep reward: 0.04\tMean ep reward with disc: 0.0108\n",
      "11:\tLoss: 1.3461\tMean ep reward: 0.03\tMean ep reward with disc: 0.0094\n",
      "12:\tLoss: 1.3408\tMean ep reward: 0.04\tMean ep reward with disc: 0.0095\n",
      "13:\tLoss: 1.3382\tMean ep reward: 0.02\tMean ep reward with disc: 0.0074\n",
      "14:\tLoss: 1.334\tMean ep reward: 0.04\tMean ep reward with disc: 0.0103\n",
      "15:\tLoss: 1.332\tMean ep reward: 0.05\tMean ep reward with disc: 0.0115\n",
      "16:\tLoss: 1.3285\tMean ep reward: 0.03\tMean ep reward with disc: 0.0121\n",
      "17:\tLoss: 1.3259\tMean ep reward: 0.01\tMean ep reward with disc: 0.0048\n",
      "18:\tLoss: 1.3208\tMean ep reward: 0.03\tMean ep reward with disc: 0.0087\n",
      "19:\tLoss: 1.3203\tMean ep reward: 0.03\tMean ep reward with disc: 0.0088\n",
      "20:\tLoss: 1.3172\tMean ep reward: 0.02\tMean ep reward with disc: 0.0073\n",
      "21:\tLoss: 1.3176\tMean ep reward: 0.02\tMean ep reward with disc: 0.0048\n",
      "22:\tLoss: 1.3138\tMean ep reward: 0.02\tMean ep reward with disc: 0.0118\n",
      "23:\tLoss: 1.3114\tMean ep reward: 0.04\tMean ep reward with disc: 0.0117\n",
      "24:\tLoss: 1.307\tMean ep reward: 0.01\tMean ep reward with disc: 0.0031\n",
      "25:\tLoss: 1.3056\tMean ep reward: 0.03\tMean ep reward with disc: 0.0069\n",
      "26:\tLoss: 1.3069\tMean ep reward: 0.07\tMean ep reward with disc: 0.0192\n",
      "27:\tLoss: 1.305\tMean ep reward: 0.03\tMean ep reward with disc: 0.0065\n",
      "28:\tLoss: 1.3025\tMean ep reward: 0.04\tMean ep reward with disc: 0.0155\n",
      "29:\tLoss: 1.2994\tMean ep reward: 0.03\tMean ep reward with disc: 0.0084\n",
      "30:\tLoss: 1.2949\tMean ep reward: 0.03\tMean ep reward with disc: 0.0141\n",
      "31:\tLoss: 1.293\tMean ep reward: 0.04\tMean ep reward with disc: 0.0141\n",
      "32:\tLoss: 1.2879\tMean ep reward: 0.04\tMean ep reward with disc: 0.0107\n",
      "33:\tLoss: 1.281\tMean ep reward: 0.06\tMean ep reward with disc: 0.0167\n",
      "34:\tLoss: 1.2759\tMean ep reward: 0.06\tMean ep reward with disc: 0.0176\n",
      "35:\tLoss: 1.2717\tMean ep reward: 0.08\tMean ep reward with disc: 0.0288\n",
      "36:\tLoss: 1.269\tMean ep reward: 0.04\tMean ep reward with disc: 0.0085\n",
      "37:\tLoss: 1.263\tMean ep reward: 0.09\tMean ep reward with disc: 0.0346\n",
      "38:\tLoss: 1.2579\tMean ep reward: 0.07\tMean ep reward with disc: 0.0191\n",
      "39:\tLoss: 1.2532\tMean ep reward: 0.07\tMean ep reward with disc: 0.0187\n",
      "40:\tLoss: 1.2471\tMean ep reward: 0.08\tMean ep reward with disc: 0.0186\n",
      "41:\tLoss: 1.2408\tMean ep reward: 0.11\tMean ep reward with disc: 0.0342\n",
      "42:\tLoss: 1.2366\tMean ep reward: 0.06\tMean ep reward with disc: 0.0172\n",
      "43:\tLoss: 1.2325\tMean ep reward: 0.07\tMean ep reward with disc: 0.0134\n",
      "44:\tLoss: 1.2291\tMean ep reward: 0.08\tMean ep reward with disc: 0.0183\n",
      "45:\tLoss: 1.2236\tMean ep reward: 0.15\tMean ep reward with disc: 0.0376\n",
      "46:\tLoss: 1.2176\tMean ep reward: 0.09\tMean ep reward with disc: 0.0282\n",
      "47:\tLoss: 1.2053\tMean ep reward: 0.17\tMean ep reward with disc: 0.0452\n",
      "48:\tLoss: 1.1962\tMean ep reward: 0.1\tMean ep reward with disc: 0.0212\n",
      "49:\tLoss: 1.188\tMean ep reward: 0.1\tMean ep reward with disc: 0.0328\n",
      "50:\tLoss: 1.1789\tMean ep reward: 0.09\tMean ep reward with disc: 0.0317\n",
      "51:\tLoss: 1.1644\tMean ep reward: 0.16\tMean ep reward with disc: 0.0346\n",
      "52:\tLoss: 1.1491\tMean ep reward: 0.18\tMean ep reward with disc: 0.053\n",
      "53:\tLoss: 1.1365\tMean ep reward: 0.13\tMean ep reward with disc: 0.0369\n",
      "54:\tLoss: 1.1172\tMean ep reward: 0.2\tMean ep reward with disc: 0.0633\n",
      "55:\tLoss: 1.1049\tMean ep reward: 0.12\tMean ep reward with disc: 0.0295\n",
      "56:\tLoss: 1.0921\tMean ep reward: 0.17\tMean ep reward with disc: 0.0476\n",
      "57:\tLoss: 1.0772\tMean ep reward: 0.21\tMean ep reward with disc: 0.0568\n",
      "58:\tLoss: 1.0552\tMean ep reward: 0.17\tMean ep reward with disc: 0.0397\n",
      "59:\tLoss: 1.0301\tMean ep reward: 0.19\tMean ep reward with disc: 0.0642\n",
      "60:\tLoss: 1.0019\tMean ep reward: 0.26\tMean ep reward with disc: 0.0775\n",
      "61:\tLoss: 0.9947\tMean ep reward: 0.21\tMean ep reward with disc: 0.0714\n",
      "62:\tLoss: 0.9616\tMean ep reward: 0.33\tMean ep reward with disc: 0.0934\n",
      "63:\tLoss: 0.9159\tMean ep reward: 0.28\tMean ep reward with disc: 0.1023\n",
      "64:\tLoss: 0.9007\tMean ep reward: 0.35\tMean ep reward with disc: 0.1171\n",
      "65:\tLoss: 0.8469\tMean ep reward: 0.3\tMean ep reward with disc: 0.1218\n",
      "66:\tLoss: 0.8353\tMean ep reward: 0.25\tMean ep reward with disc: 0.1037\n",
      "67:\tLoss: 0.8168\tMean ep reward: 0.33\tMean ep reward with disc: 0.1169\n",
      "68:\tLoss: 0.725\tMean ep reward: 0.38\tMean ep reward with disc: 0.1536\n",
      "69:\tLoss: 0.6996\tMean ep reward: 0.45\tMean ep reward with disc: 0.1773\n",
      "70:\tLoss: 0.581\tMean ep reward: 0.45\tMean ep reward with disc: 0.1906\n",
      "71:\tLoss: 0.5847\tMean ep reward: 0.41\tMean ep reward with disc: 0.1678\n",
      "72:\tLoss: 0.539\tMean ep reward: 0.48\tMean ep reward with disc: 0.2172\n",
      "73:\tLoss: 0.4108\tMean ep reward: 0.51\tMean ep reward with disc: 0.2336\n",
      "74:\tLoss: 0.4025\tMean ep reward: 0.44\tMean ep reward with disc: 0.2098\n",
      "75:\tLoss: 0.3557\tMean ep reward: 0.54\tMean ep reward with disc: 0.2457\n",
      "77:\tLoss: 0.7014\tMean ep reward: 0.6\tMean ep reward with disc: 0.294\n",
      "78:\tLoss: 0.6577\tMean ep reward: 0.55\tMean ep reward with disc: 0.2741\n",
      "79:\tLoss: 0.5793\tMean ep reward: 0.58\tMean ep reward with disc: 0.2657\n",
      "80:\tLoss: 0.4431\tMean ep reward: 0.63\tMean ep reward with disc: 0.2967\n",
      "81:\tLoss: 0.2431\tMean ep reward: 0.72\tMean ep reward with disc: 0.3731\n",
      "82:\tLoss: 0.2241\tMean ep reward: 0.67\tMean ep reward with disc: 0.3236\n",
      "84:\tLoss: 0.5547\tMean ep reward: 0.69\tMean ep reward with disc: 0.3593\n",
      "85:\tLoss: 0.3355\tMean ep reward: 0.69\tMean ep reward with disc: 0.3541\n",
      "86:\tLoss: 0.1745\tMean ep reward: 0.81\tMean ep reward with disc: 0.4106\n",
      "87:\tLoss: 0.1601\tMean ep reward: 0.69\tMean ep reward with disc: 0.3397\n",
      "89:\tLoss: 0.4445\tMean ep reward: 0.79\tMean ep reward with disc: 0.3916\n",
      "90:\tLoss: 0.2275\tMean ep reward: 0.76\tMean ep reward with disc: 0.3844\n",
      "91:\tLoss: 0.1235\tMean ep reward: 0.77\tMean ep reward with disc: 0.4089\n",
      "Environment solved!\n"
     ]
    }
   ],
   "source": [
    "# env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"human\"))\n",
    "# render_n_steps(env, None, 50)\n",
    "\n",
    "# env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False))\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\", desc=[\"SFFF\", \"FHHF\", \"FFHF\", \"HFGF\"], map_name=\"4x4\", is_slippery=False))\n",
    "\n",
    "observation_shape = env.observation_space.shape[0]\n",
    "actions_n = env.action_space.n\n",
    "    \n",
    "net = NeuralNet(observation_shape, HIDDEN_LAYER1, HIDDEN_LAYER2, actions_n)\n",
    "# net = NeuralNet(observation_shape, HIDDEN_LAYER1, actions_n)\n",
    "net = net.to(device)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(params=net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_episodes_memory = []\n",
    "\n",
    "for iteration, batch in enumerate(generate_batches_of_episodes(env, net, BATCH_SIZE, actions_n)):\n",
    "\n",
    "    mean_episode_reward = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "    mean_episode_reward_with_discount = float(np.mean(list(map(lambda s: s.reward_with_discount, batch))))\n",
    "\n",
    "    # Check the mean reward within the batch\n",
    "    if mean_episode_reward > 0.85:\n",
    "        print(\"Environment solved!\")\n",
    "        break\n",
    "\n",
    "    best_episodes_memory, batch_states_t, batch_actions_t, episode_reward_threshold = filter_batch(best_episodes_memory+batch, PERCENTILE)\n",
    "\n",
    "    if not best_episodes_memory:\n",
    "        # print(\"Skip step\")\n",
    "        continue\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    action_predictions = net(batch_states_t)\n",
    "    loss = objective(action_predictions, batch_actions_t)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    # Report performance\n",
    "    print(f\"{iteration}:\\tLoss: {round(loss.item(), 4)}\\tMean ep reward: {round(mean_episode_reward, 4)}\\tMean ep reward with disc: {round(mean_episode_reward_with_discount, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c864104-93ff-4952-a81e-be912325bb4d",
   "metadata": {},
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3e0397-cd60-4df8-b202-48ee4904a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\", desc=[\"SFFF\", \"FHHF\", \"FFHF\", \"HFGF\"], map_name=\"4x4\", is_slippery=False, render_mode=\"human\"))\n",
    "render_n_steps(env, net, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87008ef4-53e7-4103-a6ef-91c64a0e2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e20a44-2752-4289-9253-68637596c303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
